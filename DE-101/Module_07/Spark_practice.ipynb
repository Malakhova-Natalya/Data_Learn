{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ecc4e98",
   "metadata": {},
   "source": [
    "# Spark: more practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d13fda",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preparation\" data-toc-modified-id=\"Preparation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preparation</a></span></li><li><span><a href=\"#Query-with-SQL\" data-toc-modified-id=\"Query-with-SQL-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Query with SQL</a></span></li><li><span><a href=\"#Tables-and-Views\" data-toc-modified-id=\"Tables-and-Views-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Tables and Views</a></span></li><li><span><a href=\"#Spark-catalog\" data-toc-modified-id=\"Spark-catalog-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Spark catalog</a></span></li><li><span><a href=\"#Read\" data-toc-modified-id=\"Read-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Read</a></span></li><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Functions</a></span></li><li><span><a href=\"#Operations\" data-toc-modified-id=\"Operations-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Operations</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b812e3f3",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0d5e96",
   "metadata": {},
   "source": [
    "Устанавливаем необходимые библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f58a68f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.types import * \n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3192ddef",
   "metadata": {},
   "source": [
    "Задаём Спарк-сессию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7a7de4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "        .builder\n",
    "        .appName(\"SparkSQLExampleApp\")\n",
    "        .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4c77e1",
   "metadata": {},
   "source": [
    "## Query with SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4453c20",
   "metadata": {},
   "source": [
    "Задаём путь к файлу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d3f3347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data set\n",
    "csv_file = \"C:\\datasets\\departuredelays.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8ff084",
   "metadata": {},
   "source": [
    "Читаем файл и создаём временный view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5af4215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and create a temporary view\n",
    "# Infer schema (note that for larger files you \n",
    "# may want to specify the schema)\n",
    "df = (spark.read.format(\"csv\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(csv_file))\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0199d94",
   "metadata": {},
   "source": [
    "Находим все рейсы, расстояние перелёта которых составляет более 1 тысячи миль - с помощью spark.sql:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dc21a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#find all flights whose distance is greater than 1,000 miles\n",
    "spark.sql(\"\"\"SELECT distance, origin, destination \n",
    "FROM us_delay_flights_tbl WHERE distance > 1000 \n",
    "ORDER BY distance DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c0f656",
   "metadata": {},
   "source": [
    "То же самое с помощью pyspark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "916654ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Same in PySpark \n",
    "# In Python\n",
    "from pyspark.sql.functions import col, desc\n",
    "(df.select(\"distance\", \"origin\", \"destination\")\n",
    "  .where(col(\"distance\") > 1000)\n",
    "  .orderBy(desc(\"distance\"))).show(10)\n",
    "\n",
    "# Or\n",
    "(df.select(\"distance\", \"origin\", \"destination\")\n",
    "  .where(\"distance > 1000\")\n",
    "  .orderBy(\"distance\", ascending=False).show(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79baf950",
   "metadata": {},
   "source": [
    "Находим рейсы из Сан-Франциско в Чикаго, задержка которых длилась по крайней мере 2 часа:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94000b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|2190925| 1638|   SFO|        ORD|\n",
      "|1031755|  396|   SFO|        ORD|\n",
      "|1022330|  326|   SFO|        ORD|\n",
      "|1051205|  320|   SFO|        ORD|\n",
      "|1190925|  297|   SFO|        ORD|\n",
      "|2171115|  296|   SFO|        ORD|\n",
      "|1071040|  279|   SFO|        ORD|\n",
      "|1051550|  274|   SFO|        ORD|\n",
      "|3120730|  266|   SFO|        ORD|\n",
      "|1261104|  258|   SFO|        ORD|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#find all flights between San Francisco (SFO) and Chicago (ORD) with at least a two-hour delay\n",
    "spark.sql(\"\"\"SELECT date, delay, origin, destination \n",
    "FROM us_delay_flights_tbl \n",
    "WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD' \n",
    "ORDER by delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f01bde",
   "metadata": {},
   "source": [
    "Распределение всех полётов по категориям длительности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37ceceb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# label all US flights, regardless of origin and destination, with an indication of the delays they experienced: \n",
    "# Very Long Delays (> 6 hours), Long Delays (2–6 hours), etc\n",
    "spark.sql(\"\"\"SELECT delay, origin, destination, \n",
    "              CASE\n",
    "                  WHEN delay > 360 THEN 'Very Long Delays'\n",
    "                  WHEN delay >= 120 AND delay <= 360 THEN 'Long Delays'\n",
    "                  WHEN delay >= 60 AND delay < 120 THEN 'Short Delays'\n",
    "                  WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n",
    "                  WHEN delay = 0 THEN 'No Delays'\n",
    "                  ELSE 'Early'\n",
    "               END AS Flight_Delays\n",
    "               FROM us_delay_flights_tbl\n",
    "               ORDER BY origin, delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a19959",
   "metadata": {},
   "source": [
    "## Tables and Views"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f1494f",
   "metadata": {},
   "source": [
    "Создаём базу данных и указываем Спарку, что мы хотим использовать её:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e975d566",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a database called learn_spark_db and tell Spark we want to use that database\n",
    "\n",
    "#spark.sql(\"CREATE DATABASE learn_spark_db\")\n",
    "#spark.sql(\"USE learn_spark_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87413aa4",
   "metadata": {},
   "source": [
    "Создаём managed-таблицу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0482f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a managed table\n",
    "# option 1\n",
    "\n",
    "#spark.sql(\"CREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT,  \\\n",
    "#  distance INT, origin STRING, destination STRING)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3608b5f",
   "metadata": {},
   "source": [
    "Второй вариант сделать managed-таблицу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "416c455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2\n",
    "# Path to our US flight delays CSV file \n",
    "\n",
    "# csv_file = \"C:\\datasets\\departuredelays.csv\"\n",
    "\n",
    "# Schema as defined in the preceding example\n",
    "#schema=\"date STRING, delay INT, distance INT, origin STRING, destination STRING\"\n",
    "#flights_df = spark.read.csv(csv_file, schema=schema)\n",
    "#flights_df.write.saveAsTable(\"managed_us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825a7520",
   "metadata": {},
   "source": [
    "Создаём unmanaged-таблицу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2100054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an unmanaged table\n",
    "# option 1\n",
    "\n",
    "#spark.sql(\"\"\"CREATE TABLE us_delay_flights_tbl(date STRING, delay INT, \n",
    "#  distance INT, origin STRING, destination STRING) \n",
    "#  USING csv OPTIONS (PATH \n",
    "#  'departuredelays.csv')\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ac9ee8",
   "metadata": {},
   "source": [
    "Второй вариант сделать то же самое:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bafde552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2\n",
    "\n",
    "#(flights_df\n",
    "#  .write\n",
    "#  .option(\"path\", \"/tmp/data/us_flights_delay\")\n",
    "#  .saveAsTable(\"us_delay_flights_tbl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559ae0e5",
   "metadata": {},
   "source": [
    "Создаём view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dbcc0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Views\n",
    "# In Python\n",
    "\n",
    "#df_sfo = spark.sql(\"SELECT date, delay, origin, destination FROM \\\n",
    "#  us_delay_flights_tbl WHERE origin = 'SFO'\")\n",
    "    \n",
    "#df_jfk = spark.sql(\"SELECT date, delay, origin, destination FROM \\\n",
    "#  us_delay_flights_tbl WHERE origin = 'JFK'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece63f46",
   "metadata": {},
   "source": [
    "Создаём global и temporary view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad056710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary and global temporary view\n",
    "\n",
    "#df_sfo.createOrReplaceGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "#df_jfk.createOrReplaceTempView(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ff2098",
   "metadata": {},
   "source": [
    "Удаляем view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a09a42c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop views\n",
    "\n",
    "#spark.catalog.dropGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "#spark.catalog.dropTempView(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303b8d01",
   "metadata": {},
   "source": [
    "## Spark catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07fe6dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catalog\n",
    "\n",
    "#spark.catalog.dropGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "#spark.catalog.dropTempView(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0252bea",
   "metadata": {},
   "source": [
    "## Read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6607596",
   "metadata": {},
   "source": [
    "Попрактикуемся в чтении файлов разных форматов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee9d993",
   "metadata": {},
   "source": [
    "**parquet**\n",
    "\n",
    "Прочитаем файл parquet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2da6cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Parquet \n",
    "# read\n",
    "\n",
    "#file = \"C:\\datasets\\spark_summary_data\\parquet\\summary.parquet\\*\"\n",
    "#df = spark.read.format(\"parquet\").load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b76ff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write\n",
    "\n",
    "#(df.write.format(\"parquet\")\n",
    "#  .mode(\"overwrite\")\n",
    "#  .option(\"compression\", \"snappy\")\n",
    "#  .save(\"/tmp/data/parquet/df_parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b0e014",
   "metadata": {},
   "source": [
    "**csv**\n",
    "\n",
    "Следующий формат - csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16edd092",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use CSV\n",
    "# read\n",
    "\n",
    "#file = \"C:\\datasets\\spark_summary_data\\csv\\*\"\n",
    "\n",
    "#schema = \"DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count INT\"\n",
    "\n",
    "#df = (spark.read.format(\"csv\") \n",
    "#  .option(\"header\", \"true\")  \n",
    "#  .schema(schema) \n",
    "#  .option(\"mode\", \"FAILFAST\")  # Exit if any errors \n",
    "#  .option(\"nullValue\", \"\")     # Replace any null data field with quotes \n",
    "#  .load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4641b3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write\n",
    "\n",
    "#df.write.format(\"csv\").mode(\"overwrite\").save(\"/data/csv/df_csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b6f482",
   "metadata": {},
   "source": [
    "**json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fe41732",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use JSON\n",
    "# read\n",
    "\n",
    "#file = \"C:\\datasets\\spark_summary_data\\json\\*\"\n",
    "#df = spark.read.format(\"json\").load(file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da4af74a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# write\n",
    "\n",
    "#df.write.format(\"json\") \\\n",
    "#  .mode(\"overwrite\") \\\n",
    "#  .option(\"compression\", \"snappy\") \\\n",
    "#  .save(\"/tmp/data/json/df_json\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c5ed81",
   "metadata": {},
   "source": [
    "**orc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "706a9c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORC\n",
    "# read\n",
    "\n",
    "#file = \"C:\\datasets\\spark_summary_data\\orc\\*\"\n",
    "#df = spark.read.format(\"orc\").option(\"path\", file).load()\n",
    "#df.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea00194",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3745b76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"celsius\", ArrayType(IntegerType()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b661aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_list = [[35, 36, 32, 30, 40, 42, 38]], [[31, 32, 34, 55, 56]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97119ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = spark.createDataFrame(t_list, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be72c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c.createOrReplaceTempView(\"tC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "525986fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             celsius|\n",
      "+--------------------+\n",
      "|[35, 36, 32, 30, ...|\n",
      "|[31, 32, 34, 55, 56]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the DataFrame\n",
    "t_c.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "063d3758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             celsius|          fahrenheit|\n",
      "+--------------------+--------------------+\n",
      "|[35, 36, 32, 30, ...|[95, 96, 89, 86, ...|\n",
      "|[31, 32, 34, 55, 56]|[87, 89, 93, 131,...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transform()\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius, \n",
    " transform(celsius, t -> ((t * 9) div 5) + 32) as fahrenheit \n",
    "  FROM tC\n",
    "\"\"\").show()\n",
    "\n",
    "# +--------------------+--------------------+\n",
    "# |             celsius|          fahrenheit|\n",
    "# +--------------------+--------------------+\n",
    "# |[35, 36, 32, 30, ...|[95, 96, 89, 86, ...|\n",
    "# |[31, 32, 34, 55, 56]|[87, 89, 93, 131,...|\n",
    "# +--------------------+--------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "771845a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|             celsius|    high|\n",
      "+--------------------+--------+\n",
      "|[35, 36, 32, 30, ...|[40, 42]|\n",
      "|[31, 32, 34, 55, 56]|[55, 56]|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter()\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius, \n",
    " filter(celsius, t -> t > 38) as high \n",
    "  FROM tC\n",
    "\"\"\").show()\n",
    "\n",
    "# +--------------------+--------+\n",
    "# |             celsius|    high|\n",
    "# +--------------------+--------+\n",
    "# |[35, 36, 32, 30, ...|[40, 42]|\n",
    "# |[31, 32, 34, 55, 56]|[55, 56]|\n",
    "# +--------------------+--------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab6d6504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|             celsius|threshold|\n",
      "+--------------------+---------+\n",
      "|[35, 36, 32, 30, ...|     true|\n",
      "|[31, 32, 34, 55, 56]|    false|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#exists()\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius, \n",
    "       exists(celsius, t -> t = 38) as threshold\n",
    "  FROM tC\n",
    "\"\"\").show()\n",
    "\n",
    "# +--------------------+---------+\n",
    "# |             celsius|threshold|\n",
    "# +--------------------+---------+\n",
    "# |[35, 36, 32, 30, ...|     true|\n",
    "# |[31, 32, 34, 55, 56]|    false|\n",
    "# +--------------------+---------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6536c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|             celsius|avgFahrenheit|\n",
      "+--------------------+-------------+\n",
      "|[35, 36, 32, 30, ...|           96|\n",
      "|[31, 32, 34, 55, 56]|          105|\n",
      "+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reduce()\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius, \n",
    "       reduce(\n",
    "          celsius, \n",
    "          0, \n",
    "          (t, acc) -> t + acc, \n",
    "          acc -> (acc div size(celsius) * 9 div 5) + 32\n",
    "        ) as avgFahrenheit \n",
    "  FROM tC\n",
    "\"\"\").show()\n",
    "\n",
    "# +--------------------+-------------+\n",
    "# |             celsius|avgFahrenheit|\n",
    "# +--------------------+-------------+\n",
    "# |[35, 36, 32, 30, ...|           96|\n",
    "# |[31, 32, 34, 55, 56]|          105|\n",
    "# +--------------------+-------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d82cb8",
   "metadata": {},
   "source": [
    "## Operations\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10414978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file paths\n",
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a999c707",
   "metadata": {},
   "outputs": [],
   "source": [
    "tripdelaysFilePath = \"C:\\datasets\\departuredelays.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e44bbd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "airportsFilePath = \"C:\\datasets\\spark_summary_data\\codes-airport.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd98db48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain airports data set\n",
    "airports = (spark.read \n",
    "  .format(\"csv\")\n",
    "  .options(header=\"true\", inferSchema=\"true\", sep=\"\\t\")\n",
    "  .load(airportsFilePath))\n",
    "airports.createOrReplaceTempView(\"airports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0936119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain departure delays data set\n",
    "departureDelays = (spark.read \n",
    "  .format(\"csv\") \n",
    "  .options(header=\"true\") \n",
    "  .load(tripdelaysFilePath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d9a0b95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "departureDelays = (departureDelays \n",
    "  .withColumn(\"delay\", expr(\"CAST(delay as INT) as delay\")) \n",
    "  .withColumn(\"distance\", expr(\"CAST(distance as INT) as distance\")))\n",
    "departureDelays.createOrReplaceTempView(\"departureDelays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e4bbb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary small table\n",
    "foo = (departureDelays \n",
    "  .filter(expr(\"\"\"origin == 'SEA' and destination == 'SFO' and \n",
    "    date like '01010%' and delay > 0\"\"\"))) \n",
    "foo.createOrReplaceTempView(\"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "81519af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+----+\n",
      "|       City|State|Country|IATA|\n",
      "+-----------+-----+-------+----+\n",
      "| Abbotsford|   BC| Canada| YXX|\n",
      "|   Aberdeen|   SD|    USA| ABR|\n",
      "|    Abilene|   TX|    USA| ABI|\n",
      "|      Akron|   OH|    USA| CAK|\n",
      "|    Alamosa|   CO|    USA| ALS|\n",
      "|     Albany|   GA|    USA| ABY|\n",
      "|     Albany|   NY|    USA| ALB|\n",
      "|Albuquerque|   NM|    USA| ABQ|\n",
      "| Alexandria|   LA|    USA| AEX|\n",
      "|  Allentown|   PA|    USA| ABE|\n",
      "+-----------+-----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM airports LIMIT 10\").show()\n",
    "\n",
    "# +-----------+-----+-------+----+\n",
    "# |       City|State|Country|IATA|\n",
    "# +-----------+-----+-------+----+\n",
    "# | Abbotsford|   BC| Canada| YXX|\n",
    "# |   Aberdeen|   SD|    USA| ABR|\n",
    "# |    Abilene|   TX|    USA| ABI|\n",
    "# |      Akron|   OH|    USA| CAK|\n",
    "# |    Alamosa|   CO|    USA| ALS|\n",
    "# |     Albany|   GA|    USA| ABY|\n",
    "# |     Albany|   NY|    USA| ALB|\n",
    "# |Albuquerque|   NM|    USA| ABQ|\n",
    "# | Alexandria|   LA|    USA| AEX|\n",
    "# |  Allentown|   PA|    USA| ABE|\n",
    "# +-----------+-----+-------+----+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "822021f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "|01030605|    0|     602|   ABE|        ATL|\n",
      "|01041243|   10|     602|   ABE|        ATL|\n",
      "|01040605|   28|     602|   ABE|        ATL|\n",
      "|01051245|   88|     602|   ABE|        ATL|\n",
      "|01050605|    9|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM departureDelays LIMIT 10\").show()\n",
    "\n",
    "# +--------+-----+--------+------+-----------+\n",
    "# |    date|delay|distance|origin|destination|\n",
    "# +--------+-----+--------+------+-----------+\n",
    "# |01011245|    6|     602|   ABE|        ATL|\n",
    "# |01020600|   -8|     369|   ABE|        DTW|\n",
    "# |01021245|   -2|     602|   ABE|        ATL|\n",
    "# |01020605|   -4|     602|   ABE|        ATL|\n",
    "# |01031245|   -4|     602|   ABE|        ATL|\n",
    "# |01030605|    0|     602|   ABE|        ATL|\n",
    "# |01041243|   10|     602|   ABE|        ATL|\n",
    "# |01040605|   28|     602|   ABE|        ATL|\n",
    "# |01051245|   88|     602|   ABE|        ATL|\n",
    "# |01050605|    9|     602|   ABE|        ATL|\n",
    "# +--------+-----+--------+------+-----------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8ee3ce34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM foo\").show()\n",
    "\n",
    "# +--------+-----+--------+------+-----------+\n",
    "# |    date|delay|distance|origin|destination|\n",
    "# +--------+-----+--------+------+-----------+\n",
    "# |01010710|   31|     590|   SEA|        SFO|\n",
    "# |01010955|  104|     590|   SEA|        SFO|\n",
    "# |01010730|    5|     590|   SEA|        SFO|\n",
    "# +--------+-----+--------+------+-----------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2467e7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union\n",
    "# Union two tables\n",
    "bar = departureDelays.union(foo)\n",
    "bar.createOrReplaceTempView(\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "426f892f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the union (filtering for SEA and SFO in a specific time range)\n",
    "bar.filter(expr(\"\"\"origin == 'SEA' AND destination == 'SFO'\n",
    "AND date LIKE '01010%' AND delay > 0\"\"\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "78d4d589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * \n",
    "  FROM bar \n",
    " WHERE origin = 'SEA' \n",
    "   AND destination = 'SFO' \n",
    "   AND date LIKE '01010%' \n",
    "   AND delay > 0\n",
    "\"\"\").show()\n",
    "\n",
    "# +--------+-----+--------+------+-----------+\n",
    "# |    date|delay|distance|origin|destination|\n",
    "# +--------+-----+--------+------+-----------+\n",
    "# |01010710|   31|     590|   SEA|        SFO|\n",
    "# |01010955|  104|     590|   SEA|        SFO|\n",
    "# |01010730|    5|     590|   SEA|        SFO|\n",
    "# |01010710|   31|     590|   SEA|        SFO|\n",
    "# |01010955|  104|     590|   SEA|        SFO|\n",
    "# |01010730|    5|     590|   SEA|        SFO|\n",
    "# +--------+-----+--------+------+-----------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "02064960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|   City|State|    date|delay|distance|destination|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|Seattle|   WA|01010710|   31|     590|        SFO|\n",
      "|Seattle|   WA|01010955|  104|     590|        SFO|\n",
      "|Seattle|   WA|01010730|    5|     590|        SFO|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Joins\n",
    "foo.join(\n",
    "  airports, \n",
    "  airports.IATA == foo.origin\n",
    ").select(\"City\", \"State\", \"date\", \"delay\", \"distance\", \"destination\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0983cfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|   City|State|    date|delay|distance|destination|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|Seattle|   WA|01010710|   31|     590|        SFO|\n",
      "|Seattle|   WA|01010955|  104|     590|        SFO|\n",
      "|Seattle|   WA|01010730|    5|     590|        SFO|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT a.City, a.State, f.date, f.delay, f.distance, f.destination \n",
    "  FROM foo f\n",
    "  JOIN airports a\n",
    "    ON a.IATA = f.origin\n",
    "\"\"\").show()\n",
    "\n",
    "# +-------+-----+--------+-----+--------+-----------+\n",
    "# |   City|State|    date|delay|distance|destination|\n",
    "# +-------+-----+--------+-----+--------+-----------+\n",
    "# |Seattle|   WA|01010710|   31|     590|        SFO|\n",
    "# |Seattle|   WA|01010955|  104|     590|        SFO|\n",
    "# |Seattle|   WA|01010730|    5|     590|        SFO|\n",
    "# +-------+-----+--------+-----+--------+-----------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a7d903ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+-------+\n",
      "|    date|delay|distance|origin|destination| status|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "|01010710|   31|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|  104|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|    5|     590|   SEA|        SFO|On-time|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Modifications\n",
    "from pyspark.sql.functions import expr\n",
    "foo2 = (foo.withColumn(\n",
    "          \"status\", \n",
    "          expr(\"CASE WHEN delay <= 10 THEN 'On-time' ELSE 'Delayed' END\")\n",
    "        ))\n",
    "\n",
    "foo2.show()\n",
    "\n",
    "# +--------+-----+--------+------+-----------+-------+\n",
    "# |    date|delay|distance|origin|destination| status|\n",
    "# +--------+-----+--------+------+-----------+-------+\n",
    "# |01010710|   31|     590|   SEA|        SFO|Delayed|\n",
    "# |01010955|  104|     590|   SEA|        SFO|Delayed|\n",
    "# |01010730|    5|     590|   SEA|        SFO|On-time|\n",
    "# +--------+-----+--------+------+-----------+-------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d817ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------+\n",
      "|    date|distance|origin|destination| status|\n",
      "+--------+--------+------+-----------+-------+\n",
      "|01010710|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|     590|   SEA|        SFO|On-time|\n",
      "+--------+--------+------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo3 = foo2.drop(\"delay\")\n",
    "foo3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "99221f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------------+\n",
      "|    date|distance|origin|destination|flight_status|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "|01010710|     590|   SEA|        SFO|      Delayed|\n",
      "|01010955|     590|   SEA|        SFO|      Delayed|\n",
      "|01010730|     590|   SEA|        SFO|      On-time|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renamning columns\n",
    "foo4 = foo3.withColumnRenamed(\"status\", \"flight_status\")\n",
    "foo4.show()\n",
    "\n",
    "# +--------+--------+------+-----------+-------------+\n",
    "# |    date|distance|origin|destination|flight_status|\n",
    "# +--------+--------+------+-----------+-------------+\n",
    "# |01010710|     590|   SEA|        SFO|      Delayed|\n",
    "# |01010955|     590|   SEA|        SFO|      Delayed|\n",
    "# |01010730|     590|   SEA|        SFO|      On-time|\n",
    "# +--------+--------+------+-----------+-------------+\n",
    "\n",
    "# +--------+--------+------+-----------+-------+\n",
    "# |    date|distance|origin|destination| status|\n",
    "# +--------+--------+------+-----------+-------+\n",
    "# |01010710|     590|   SEA|        SFO|Delayed|\n",
    "# |01010955|     590|   SEA|        SFO|Delayed|\n",
    "# |01010730|     590|   SEA|        SFO|On-time|\n",
    "# +--------+--------+------+-----------+-------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d5fa25f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+------------+\n",
      "|destination|JAN_AvgDelay|JAN_MaxDelay|FEB_AvgDelay|FEB_MaxDelay|\n",
      "+-----------+------------+------------+------------+------------+\n",
      "|        ABQ|       19.86|         316|       11.42|          69|\n",
      "|        ANC|        4.44|         149|        7.90|         141|\n",
      "|        ATL|       11.98|         397|        7.73|         145|\n",
      "|        AUS|        3.48|          50|       -0.21|          18|\n",
      "|        BOS|        7.84|         110|       14.58|         152|\n",
      "|        BUR|       -2.03|          56|       -1.89|          78|\n",
      "|        CLE|       16.00|          27|        NULL|        NULL|\n",
      "|        CLT|        2.53|          41|       12.96|         228|\n",
      "|        COS|        5.32|          82|       12.18|         203|\n",
      "|        CVG|       -0.50|           4|        NULL|        NULL|\n",
      "|        DCA|       -1.15|          50|        0.07|          34|\n",
      "|        DEN|       13.13|         425|       12.95|         625|\n",
      "|        DFW|        7.95|         247|       12.57|         356|\n",
      "|        DTW|        9.18|         107|        3.47|          77|\n",
      "|        EWR|        9.63|         236|        5.20|         212|\n",
      "|        FAI|        1.84|         160|        4.21|          60|\n",
      "|        FAT|        1.36|         119|        5.22|         232|\n",
      "|        FLL|        2.94|          54|        3.50|          40|\n",
      "|        GEG|        2.28|          63|        2.87|          60|\n",
      "|        HDN|       -0.44|          27|       -6.50|           0|\n",
      "+-----------+------------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pivoting in SQL\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM (\n",
    "SELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay \n",
    "  FROM departureDelays WHERE origin = 'SEA' \n",
    ") \n",
    "PIVOT (\n",
    "  CAST(AVG(delay) AS DECIMAL(4, 2)) AS AvgDelay, MAX(delay) AS MaxDelay\n",
    "  FOR month IN (1 JAN, 2 FEB)\n",
    ")\n",
    "ORDER BY destination\n",
    "\"\"\").show()\n",
    "\n",
    "# +-----------+------------+------------+------------+------------+\n",
    "# |destination|JAN_AvgDelay|JAN_MaxDelay|FEB_AvgDelay|FEB_MaxDelay|\n",
    "# +-----------+------------+------------+------------+------------+\n",
    "# |        ABQ|       19.86|         316|       11.42|          69|\n",
    "# |        ANC|        4.44|         149|        7.90|         141|\n",
    "# |        ATL|       11.98|         397|        7.73|         145|\n",
    "# |        AUS|        3.48|          50|       -0.21|          18|\n",
    "# |        BOS|        7.84|         110|       14.58|         152|\n",
    "# |        BUR|       -2.03|          56|       -1.89|          78|\n",
    "# |        CLE|       16.00|          27|        null|        null|\n",
    "# |        CLT|        2.53|          41|       12.96|         228|\n",
    "# |        COS|        5.32|          82|       12.18|         203|\n",
    "# |        CVG|       -0.50|           4|        null|        null|\n",
    "# |        DCA|       -1.15|          50|        0.07|          34|\n",
    "# |        DEN|       13.13|         425|       12.95|         625|\n",
    "# |        DFW|        7.95|         247|       12.57|         356|\n",
    "# |        DTW|        9.18|         107|        3.47|          77|\n",
    "# |        EWR|        9.63|         236|        5.20|         212|\n",
    "# |        FAI|        1.84|         160|        4.21|          60|\n",
    "# |        FAT|        1.36|         119|        5.22|         232|\n",
    "# |        FLL|        2.94|          54|        3.50|          40|\n",
    "# |        GEG|        2.28|          63|        2.87|          60|\n",
    "# |        HDN|       -0.44|          27|       -6.50|           0|\n",
    "# +-----------+------------+------------+------------+------------+"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Содержание",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
